{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -Welcome to our Twitter Bot Topic Modeler-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237487, 2)\n",
      "Index(['labels', 'text'], dtype='object')\n",
      "  labels                                               text\n",
      "0    bot  @aprilPINKie Justice isn’t going anywhere right?!\n",
      "1    bot  Last game at DKR in 2022! Longhorns for Christ...\n",
      "2    bot  @CFBONFOX Texas goes 4-0 and makes the big 12 ...\n",
      "3    bot                      @hookemcowboys It’s from 2020\n",
      "4    bot  @CFBONFOX The Texas Longhorns. Nothing better ...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Remove non-bot (human) tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14581, 2)\n",
      "  labels                                               text\n",
      "0    bot  @aprilPINKie Justice isn’t going anywhere right?!\n",
      "1    bot  Last game at DKR in 2022! Longhorns for Christ...\n",
      "2    bot  @CFBONFOX Texas goes 4-0 and makes the big 12 ...\n",
      "3    bot                      @hookemcowboys It’s from 2020\n",
      "4    bot  @CFBONFOX The Texas Longhorns. Nothing better ...\n"
     ]
    }
   ],
   "source": [
    "bot_tweets = df[(df['labels'] == 'bot')]\n",
    "print(bot_tweets.shape)\n",
    "print(bot_tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning\n",
    "Here we will tidy up the tweets. Remove hashtags, links, @mentions, and any punctuation and emojis and marking if a tweet is a retweet. We will also be converting those words into to their root form in the process called rooting/lemmatization.\n",
    "\n",
    "Our topic model needs need these things removed before converting it into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import emoji # python3 -m pip install emoji --upgrade\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# run the following if you get an SSL error\n",
    "# \"/Applications/Python 3.10/Install Certificates.command\"\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "#patterns\n",
    "mentions_pattern =re.compile(r'@\\w*')\n",
    "url_pattern = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "hashtags_pattern = re.compile(r'#\\w*')\n",
    "reserved_words_pattern = re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n",
    "empty_spaces = re.compile(r'\\s{2,}|\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(text):\n",
    "\n",
    "    text = str(text) + \"\"\n",
    "    #set text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove mentions\n",
    "    text = re.sub(pattern=mentions_pattern, repl='', string=text)\n",
    "\n",
    "    #remove urls\n",
    "    text = re.sub(pattern=url_pattern, repl='', string=text)\n",
    "\n",
    "    #remove links\n",
    "    text = re.sub(r'http(s)?\\S+', '', text)\n",
    "\n",
    "    #remove @ tags of users\n",
    "    text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove retweet\n",
    "    text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)  # remove tweeted at\n",
    "    \n",
    "    #remove twitter reserved_words\n",
    "    text = re.sub(pattern=reserved_words_pattern, repl='', string=text)\n",
    "\n",
    "    #remove hashtags\n",
    "    #text = re.sub(pattern=hashtags_pattern, repl='', string=text)\n",
    "\n",
    "    #remove emojis\n",
    "    text = emoji.replace_emoji(text, \"\")\n",
    "\n",
    "    #remove words with less than 3 characters\n",
    "    text_list = text.split(' ')\n",
    "    for x in text_list:\n",
    "        if len(x) <= 2:\n",
    "            text_list.remove(x)\n",
    "    text = ' '.join(text_list)\n",
    "\n",
    "    #remove stopwords and apply rooting\n",
    "    if len(text) > 0:\n",
    "        text = \"the \" + text\n",
    "        newText = [word for word in text.split(' ')\n",
    "                            if word not in stopWords]\n",
    "        newText = [word_rooter(word) if '#' not in word else word\n",
    "                            for word in newText]\n",
    "        text = ' '.join(newText)\n",
    "\n",
    "    #remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) #this removes hashtag symbols as well\n",
    "    text = re.sub('[' + '!\"$%&\\'’()*+,-./:;<=>?[\\\\]^_`{|}~•@.\"\"-,`' + ']+', '', text)\n",
    "\n",
    "    #remove numbers\n",
    "    text_list = text.split(' ')\n",
    "    for x in text_list:\n",
    "        if x.isnumeric():\n",
    "                text_list.remove(x)\n",
    "    text = ' '.join(text_list)\n",
    "    #text = re.sub('([0-9]+)', '', text)\n",
    "        \n",
    "    #remove words with 2 or less characters\n",
    "    text_list = text.split(' ')\n",
    "    for x in text_list:\n",
    "        if len(x) <= 2:\n",
    "            text_list.remove(x)\n",
    "    text = ' '.join(text_list)\n",
    "\n",
    "    #remove empty spaces\n",
    "    text = re.sub(pattern=empty_spaces, repl=' ', string=text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    #remove empty lines\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13599, 3)\n",
      "       labels                                               text  \\\n",
      "0         bot  @aprilPINKie Justice isn’t going anywhere right?!   \n",
      "1         bot  Last game at DKR in 2022! Longhorns for Christ...   \n",
      "2         bot  @CFBONFOX Texas goes 4-0 and makes the big 12 ...   \n",
      "4         bot  @CFBONFOX The Texas Longhorns. Nothing better ...   \n",
      "5         bot  @UTBarstool Unfortunately we don’t play for tw...   \n",
      "...       ...                                                ...   \n",
      "226413    bot  @prof_mirya Why doesn’t gd give us a November ...   \n",
      "226414    bot  @SumitaPahwa @alixabeth Absolutely lovely Xmas...   \n",
      "226416    bot  @prof_mirya I would watch a demo of you packin...   \n",
      "226417    bot  @prof_mirya “ Dream of living out in the woods...   \n",
      "226418    bot                         @dannagal It’s the smolder   \n",
      "\n",
      "                                       cleaned_bot_tweets  \n",
      "0                               justic isnt anywher right  \n",
      "1       last game dkr longhorn christ park lot open…so...  \n",
      "2       texa goe make big championship end rematch bam...  \n",
      "4                  texa longhorns noth better burnt orang  \n",
      "5                               unfoun dont play two week  \n",
      "...                                                   ...  \n",
      "226413              doesnt give novemb august realli need  \n",
      "226414  absolut love xma tin treats bought one everyth...  \n",
      "226416                    would watch demo pack week case  \n",
      "226417   dream live woods alone years” oooh feel seeeeeen  \n",
      "226418                                            smolder  \n",
      "\n",
      "[13599 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/2g79_8515n92gvcpk3p50t880000gn/T/ipykernel_15758/362370051.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bot_tweets['cleaned_bot_tweets']  = bot_tweets['text'].apply(lambda x: tweet_cleaner(x))\n"
     ]
    }
   ],
   "source": [
    "bot_tweets['cleaned_bot_tweets']  = bot_tweets['text'].apply(lambda x: tweet_cleaner(x))\n",
    "bot_tweets = bot_tweets[(bot_tweets['cleaned_bot_tweets'].str.len() > 2)]\n",
    "print(bot_tweets.shape)\n",
    "print(bot_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13599, 1)\n",
      "                                  cleaned_bot_tweets\n",
      "0                          justic isnt anywher right\n",
      "1  last game dkr longhorn christ park lot open…so...\n",
      "2  texa goe make big championship end rematch bam...\n",
      "4             texa longhorns noth better burnt orang\n",
      "5                          unfoun dont play two week\n"
     ]
    }
   ],
   "source": [
    "bot_tweets.drop(columns='text', inplace=True)\n",
    "bot_tweets.drop(columns='labels', inplace=True)\n",
    "print(bot_tweets.shape)\n",
    "print(bot_tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Topic Modelling\n",
    "In this stage we will build a fitted topic model for the bot tweets. Model will be our LDA (Latent Dirichlet Allocation) algorithm model object which will holds parameters such as the number of topics that we provided it when we made it. Model also it also stores functions such as the fitting method. Once it is fit it will store fitted parameters that will tell us how valuable different words are in various topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets take a look at the most repeated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12436 unique tweets.\n",
      "Most repeated tweets:\n",
      "                                      cleaned_bot_tweets  count\n",
      "8448   s4s lt3 view free sex video collection updat e...     47\n",
      "6093   lt3 view free sex video collection updat every...     46\n",
      "6934                                                nice     42\n",
      "2932   environment plan essenti oper everi project he...     40\n",
      "2009                                     congratulations     37\n",
      "9637                                               thank     37\n",
      "9855                                           thank you     33\n",
      "5296   join intern conferec upadsd cairo egypt abstra...     30\n",
      "1953                                           congratul     30\n",
      "11463                                                wow     25\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(bot_tweets[\"cleaned_bot_tweets\"].unique().shape[0]) + \" unique tweets.\")\n",
    "most_repeated_tweets = bot_tweets.groupby(['cleaned_bot_tweets']).size().reset_index(name='count').sort_values('count', ascending=False).head(10)\n",
    "print(\"Most repeated tweets:\")\n",
    "print(most_repeated_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1 list</th>\n",
       "      <th>T1 weights</th>\n",
       "      <th>Topic 2 list</th>\n",
       "      <th>T2 weights</th>\n",
       "      <th>Topic 3 list</th>\n",
       "      <th>T3 weights</th>\n",
       "      <th>Topic 4 list</th>\n",
       "      <th>T4 weights</th>\n",
       "      <th>Topic 5 list</th>\n",
       "      <th>T5 weights</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic 11 list</th>\n",
       "      <th>T11 weights</th>\n",
       "      <th>Topic 12 list</th>\n",
       "      <th>T12 weights</th>\n",
       "      <th>Topic 13 list</th>\n",
       "      <th>T13 weights</th>\n",
       "      <th>Topic 14 list</th>\n",
       "      <th>T14 weights</th>\n",
       "      <th>Topic 15 list</th>\n",
       "      <th>T15 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aificialintelligence</td>\n",
       "      <td>169.1</td>\n",
       "      <td>one</td>\n",
       "      <td>403.8</td>\n",
       "      <td>video</td>\n",
       "      <td>319.1</td>\n",
       "      <td>paper</td>\n",
       "      <td>393.8</td>\n",
       "      <td>new</td>\n",
       "      <td>378.4</td>\n",
       "      <td>...</td>\n",
       "      <td>help</td>\n",
       "      <td>172.4</td>\n",
       "      <td>use</td>\n",
       "      <td>431.0</td>\n",
       "      <td>like</td>\n",
       "      <td>342.0</td>\n",
       "      <td>would</td>\n",
       "      <td>246.9</td>\n",
       "      <td>get</td>\n",
       "      <td>430.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech</td>\n",
       "      <td>169.1</td>\n",
       "      <td>day</td>\n",
       "      <td>336.7</td>\n",
       "      <td>free</td>\n",
       "      <td>229.1</td>\n",
       "      <td>accept</td>\n",
       "      <td>172.1</td>\n",
       "      <td>year</td>\n",
       "      <td>238.3</td>\n",
       "      <td>...</td>\n",
       "      <td>everi</td>\n",
       "      <td>147.1</td>\n",
       "      <td>que</td>\n",
       "      <td>284.1</td>\n",
       "      <td>peopl</td>\n",
       "      <td>251.8</td>\n",
       "      <td>think</td>\n",
       "      <td>238.4</td>\n",
       "      <td>find</td>\n",
       "      <td>218.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloud</td>\n",
       "      <td>155.0</td>\n",
       "      <td>hope</td>\n",
       "      <td>169.3</td>\n",
       "      <td>lt3</td>\n",
       "      <td>217.1</td>\n",
       "      <td>full</td>\n",
       "      <td>154.1</td>\n",
       "      <td>see</td>\n",
       "      <td>227.6</td>\n",
       "      <td>...</td>\n",
       "      <td>project</td>\n",
       "      <td>146.8</td>\n",
       "      <td>para</td>\n",
       "      <td>161.1</td>\n",
       "      <td>make</td>\n",
       "      <td>243.1</td>\n",
       "      <td>want</td>\n",
       "      <td>203.9</td>\n",
       "      <td>still</td>\n",
       "      <td>215.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robot</td>\n",
       "      <td>132.1</td>\n",
       "      <td>del</td>\n",
       "      <td>127.1</td>\n",
       "      <td>view</td>\n",
       "      <td>140.1</td>\n",
       "      <td>science</td>\n",
       "      <td>130.1</td>\n",
       "      <td>pleas</td>\n",
       "      <td>190.8</td>\n",
       "      <td>...</td>\n",
       "      <td>link</td>\n",
       "      <td>146.3</td>\n",
       "      <td>vegaswap</td>\n",
       "      <td>141.1</td>\n",
       "      <td>way</td>\n",
       "      <td>223.4</td>\n",
       "      <td>even</td>\n",
       "      <td>152.8</td>\n",
       "      <td>know</td>\n",
       "      <td>138.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology</td>\n",
       "      <td>117.3</td>\n",
       "      <td>code</td>\n",
       "      <td>124.0</td>\n",
       "      <td>updat</td>\n",
       "      <td>136.9</td>\n",
       "      <td>research</td>\n",
       "      <td>129.8</td>\n",
       "      <td>look</td>\n",
       "      <td>179.8</td>\n",
       "      <td>...</td>\n",
       "      <td>happi</td>\n",
       "      <td>137.0</td>\n",
       "      <td>crypto</td>\n",
       "      <td>134.1</td>\n",
       "      <td>that</td>\n",
       "      <td>161.2</td>\n",
       "      <td>train</td>\n",
       "      <td>144.1</td>\n",
       "      <td>now</td>\n",
       "      <td>137.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iot</td>\n",
       "      <td>110.1</td>\n",
       "      <td>nice</td>\n",
       "      <td>122.1</td>\n",
       "      <td>work</td>\n",
       "      <td>129.7</td>\n",
       "      <td>good</td>\n",
       "      <td>129.2</td>\n",
       "      <td>work</td>\n",
       "      <td>171.0</td>\n",
       "      <td>...</td>\n",
       "      <td>plan</td>\n",
       "      <td>122.5</td>\n",
       "      <td>blockchain</td>\n",
       "      <td>113.1</td>\n",
       "      <td>take</td>\n",
       "      <td>160.7</td>\n",
       "      <td>model</td>\n",
       "      <td>122.5</td>\n",
       "      <td>let</td>\n",
       "      <td>119.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ai</td>\n",
       "      <td>107.1</td>\n",
       "      <td>today</td>\n",
       "      <td>104.1</td>\n",
       "      <td>sex</td>\n",
       "      <td>127.1</td>\n",
       "      <td>publish</td>\n",
       "      <td>127.1</td>\n",
       "      <td>data</td>\n",
       "      <td>163.8</td>\n",
       "      <td>...</td>\n",
       "      <td>may</td>\n",
       "      <td>120.5</td>\n",
       "      <td>defi</td>\n",
       "      <td>108.1</td>\n",
       "      <td>talk</td>\n",
       "      <td>157.1</td>\n",
       "      <td>like</td>\n",
       "      <td>118.5</td>\n",
       "      <td>come</td>\n",
       "      <td>114.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>machinelearning</td>\n",
       "      <td>106.1</td>\n",
       "      <td>first</td>\n",
       "      <td>95.9</td>\n",
       "      <td>right</td>\n",
       "      <td>119.7</td>\n",
       "      <td>2022</td>\n",
       "      <td>125.1</td>\n",
       "      <td>excit</td>\n",
       "      <td>155.1</td>\n",
       "      <td>...</td>\n",
       "      <td>confer</td>\n",
       "      <td>119.4</td>\n",
       "      <td>cryptocurrency</td>\n",
       "      <td>102.1</td>\n",
       "      <td>dont</td>\n",
       "      <td>130.9</td>\n",
       "      <td>time</td>\n",
       "      <td>103.9</td>\n",
       "      <td>work</td>\n",
       "      <td>114.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>con</td>\n",
       "      <td>105.2</td>\n",
       "      <td>orang</td>\n",
       "      <td>93.1</td>\n",
       "      <td>everyday</td>\n",
       "      <td>112.1</td>\n",
       "      <td>web</td>\n",
       "      <td>119.1</td>\n",
       "      <td>share</td>\n",
       "      <td>151.1</td>\n",
       "      <td>...</td>\n",
       "      <td>meet</td>\n",
       "      <td>119.1</td>\n",
       "      <td>market</td>\n",
       "      <td>99.5</td>\n",
       "      <td>life</td>\n",
       "      <td>130.9</td>\n",
       "      <td>move</td>\n",
       "      <td>92.1</td>\n",
       "      <td>this</td>\n",
       "      <td>109.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cloudcomputing</td>\n",
       "      <td>100.1</td>\n",
       "      <td>news</td>\n",
       "      <td>88.9</td>\n",
       "      <td>improv</td>\n",
       "      <td>106.4</td>\n",
       "      <td>phd</td>\n",
       "      <td>119.1</td>\n",
       "      <td>present</td>\n",
       "      <td>147.1</td>\n",
       "      <td>...</td>\n",
       "      <td>use</td>\n",
       "      <td>115.0</td>\n",
       "      <td>vga</td>\n",
       "      <td>86.1</td>\n",
       "      <td>need</td>\n",
       "      <td>130.5</td>\n",
       "      <td>know</td>\n",
       "      <td>86.3</td>\n",
       "      <td>play</td>\n",
       "      <td>100.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Topic 1 list T1 weights Topic 2 list T2 weights Topic 3 list  \\\n",
       "0  aificialintelligence      169.1          one      403.8        video   \n",
       "1                  tech      169.1          day      336.7         free   \n",
       "2                 cloud      155.0         hope      169.3          lt3   \n",
       "3                 robot      132.1          del      127.1         view   \n",
       "4            technology      117.3         code      124.0        updat   \n",
       "5                   iot      110.1         nice      122.1         work   \n",
       "6                    ai      107.1        today      104.1          sex   \n",
       "7       machinelearning      106.1        first       95.9        right   \n",
       "8                   con      105.2        orang       93.1     everyday   \n",
       "9        cloudcomputing      100.1         news       88.9       improv   \n",
       "\n",
       "  T3 weights Topic 4 list T4 weights Topic 5 list T5 weights  ...  \\\n",
       "0      319.1        paper      393.8          new      378.4  ...   \n",
       "1      229.1       accept      172.1         year      238.3  ...   \n",
       "2      217.1         full      154.1          see      227.6  ...   \n",
       "3      140.1      science      130.1        pleas      190.8  ...   \n",
       "4      136.9     research      129.8         look      179.8  ...   \n",
       "5      129.7         good      129.2         work      171.0  ...   \n",
       "6      127.1      publish      127.1         data      163.8  ...   \n",
       "7      119.7         2022      125.1        excit      155.1  ...   \n",
       "8      112.1          web      119.1        share      151.1  ...   \n",
       "9      106.4          phd      119.1      present      147.1  ...   \n",
       "\n",
       "  Topic 11 list T11 weights   Topic 12 list T12 weights Topic 13 list  \\\n",
       "0          help       172.4             use       431.0          like   \n",
       "1         everi       147.1             que       284.1         peopl   \n",
       "2       project       146.8            para       161.1          make   \n",
       "3          link       146.3        vegaswap       141.1           way   \n",
       "4         happi       137.0          crypto       134.1          that   \n",
       "5          plan       122.5      blockchain       113.1          take   \n",
       "6           may       120.5            defi       108.1          talk   \n",
       "7        confer       119.4  cryptocurrency       102.1          dont   \n",
       "8          meet       119.1          market        99.5          life   \n",
       "9           use       115.0             vga        86.1          need   \n",
       "\n",
       "  T13 weights Topic 14 list T14 weights Topic 15 list T15 weights  \n",
       "0       342.0         would       246.9           get       430.5  \n",
       "1       251.8         think       238.4          find       218.5  \n",
       "2       243.1          want       203.9         still       215.8  \n",
       "3       223.4          even       152.8          know       138.1  \n",
       "4       161.2         train       144.1           now       137.1  \n",
       "5       160.7         model       122.5           let       119.5  \n",
       "6       157.1          like       118.5          come       114.4  \n",
       "7       130.9          time       103.9          work       114.3  \n",
       "8       130.9          move        92.1          this       109.1  \n",
       "9       130.5          know        86.3          play       100.5  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#highest max_df is 0.045\n",
    "#lowest min_df is ...\n",
    "#Research shows that 25% of blog posts are made up of stop words so well set max_df to 0.25\n",
    "vectorizer = CountVectorizer(max_df=0.25, min_df=0.005, token_pattern='\\w+|\\$[\\d\\.]+|\\S+') # used to transform text to vector form\n",
    "\n",
    "dtm = vectorizer.fit_transform(bot_tweets[\"cleaned_bot_tweets\"]).toarray()\n",
    "\n",
    "featureNames = vectorizer.get_feature_names_out() # what word each column in the matric represents\n",
    "\n",
    "numberOfTopicsToDisplay = 15\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=numberOfTopicsToDisplay, random_state=0)\n",
    "\n",
    "model.fit(dtm)\n",
    "\n",
    "def print_topics(model, featureNames, noTopWords):\n",
    "    topic_map = {}\n",
    "    for topicIdx, topic in enumerate(model.components_):\n",
    "        topic_map[\"Topic %d list\" % (topicIdx + 1)]= ['{}'.format(featureNames[i])\n",
    "                        for i in topic.argsort()[:-noTopWords - 1:-1]]\n",
    "        topic_map[\"T%d weights\" % (topicIdx + 1)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-noTopWords - 1:-1]]\n",
    "    return pd.DataFrame(topic_map)\n",
    "\n",
    "noTopWords = 10\n",
    "print_topics(model, featureNames, noTopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
